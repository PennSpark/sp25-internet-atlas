{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b17d9ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "p = \"/Users/estellekim/Downloads/web_routineness_release/raw/browsing.csv\"\n",
    "df = pd.read_csv(p)\n",
    "df[df[\"panelist_id\"].between(0, 7)].to_csv(p.replace(\".csv\",\"_panelist0_7.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a53754f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/estellekim/miniconda3/lib/python3.13/site-packages (2.2.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"/Users/estellekim/Downloads/web_routineness_release/raw/browsing_panelist0_7.csv\")  # has columns: domain, subdomain\n",
    "\n",
    "sub = df[\"subdomain\"].astype(str).str.strip().str.strip(\".\")\n",
    "dom = df[\"domain\"].astype(str).str.strip().str.strip(\".\")\n",
    "\n",
    "df[\"full_domain\"] = np.where(sub.eq(\"\") | sub.eq(\"nan\"),\n",
    "                             dom,\n",
    "                             sub + \".\" + dom)\n",
    "\n",
    "df.to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74cb38c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               used_at           used_at_iso  used_at_unix        date  hour  \\\n",
      "0  2018-10-05 01:48:03  2018-10-05T01:48:03Z    1538704083  2018-10-05     1   \n",
      "1  2018-10-05 01:38:57  2018-10-05T01:38:57Z    1538703537  2018-10-05     1   \n",
      "2  2018-10-05 00:06:27  2018-10-05T00:06:27Z    1538697987  2018-10-05     0   \n",
      "3  2018-10-05 00:00:27  2018-10-05T00:00:27Z    1538697627  2018-10-05     0   \n",
      "4  2018-10-05 01:08:23  2018-10-05T01:08:23Z    1538701703  2018-10-05     1   \n",
      "\n",
      "  weekday  \n",
      "0  Friday  \n",
      "1  Friday  \n",
      "2  Friday  \n",
      "3  Friday  \n",
      "4  Friday  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "IN = \"output.csv\"\n",
    "df = pd.read_csv(IN)\n",
    "\n",
    "# Parse the timestamp column\n",
    "df[\"used_at_dt\"] = pd.to_datetime(df[\"used_at\"], errors=\"coerce\")\n",
    "\n",
    "# If these timestamps are UTC (common), set tz:\n",
    "df[\"used_at_utc\"] = df[\"used_at_dt\"].dt.tz_localize(\"UTC\")  # drop this if already tz-aware\n",
    "\n",
    "# If they are local time in, say, America/New_York, localize instead:\n",
    "# df[\"used_at_local\"] = pd.to_datetime(df[\"used_at\"], errors=\"coerce\").dt.tz_localize(\"America/New_York\")\n",
    "\n",
    "# Derivatives\n",
    "df[\"used_at_iso\"]   = df[\"used_at_utc\"].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")     # ISO-8601\n",
    "df[\"used_at_unix\"]  = df[\"used_at_utc\"].astype(\"int64\") // 10**9              # epoch seconds\n",
    "df[\"date\"]          = df[\"used_at_utc\"].dt.date\n",
    "df[\"hour\"]          = df[\"used_at_utc\"].dt.hour\n",
    "df[\"weekday\"]       = df[\"used_at_utc\"].dt.day_name()\n",
    "\n",
    "# If you computed end_time earlier and want epoch too:\n",
    "# df[\"end_time_unix\"] = df[\"end_time\"].dt.tz_localize(\"UTC\").astype(\"int64\") // 10**9\n",
    "\n",
    "print(df[[\"used_at\",\"used_at_iso\",\"used_at_unix\",\"date\",\"hour\",\"weekday\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa717a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote output_collapsed_iso.csv (10482 rows)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "IN  = \"output.csv\"                               # your original file\n",
    "OUT = IN.replace(\".csv\", \"_collapsed_iso.csv\")\n",
    "\n",
    "USER = \"panelist_id\"\n",
    "TS   = \"used_at\"\n",
    "DUR  = \"active_seconds\"\n",
    "FDOM = \"full_domain\"\n",
    "\n",
    "# Load & parse\n",
    "df = pd.read_csv(IN)\n",
    "df[TS]  = pd.to_datetime(df[TS], errors=\"coerce\")\n",
    "df[DUR] = pd.to_numeric(df[DUR], errors=\"coerce\").fillna(0).astype(float)\n",
    "\n",
    "# Sort so \"consecutive\" has a clear meaning (per user, chronological)\n",
    "df = df.sort_values([USER, TS], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "# Per-row end time (used for true run end)\n",
    "df[\"_row_end\"] = df[TS] + pd.to_timedelta(df[DUR], unit=\"s\")\n",
    "\n",
    "# New run starts when user or domain changes from previous row\n",
    "is_new_run = (df[USER].ne(df[USER].shift())) | (df[FDOM].ne(df[FDOM].shift()))\n",
    "df[\"_run_id\"] = is_new_run.cumsum()\n",
    "\n",
    "# Aggregate each consecutive run\n",
    "collapsed = (\n",
    "    df.groupby([USER, FDOM, \"_run_id\"], as_index=False)\n",
    "      .agg(\n",
    "          start_time=(TS, \"first\"),\n",
    "          end_time=(\"_row_end\", \"max\"),\n",
    "          total_active_seconds=(DUR, \"sum\"),\n",
    "          row_count=(\"_run_id\", \"size\"),\n",
    "      )\n",
    ")\n",
    "\n",
    "# Replace time columns with ISO-8601 UTC strings (easy for frontend)\n",
    "collapsed[\"start_time\"] = collapsed[\"start_time\"].dt.tz_localize(\"UTC\").dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "collapsed[\"end_time\"]   = collapsed[\"end_time\"].dt.tz_localize(\"UTC\").dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "# Final columns and sort by person then time\n",
    "collapsed = collapsed[[USER, FDOM, \"start_time\", \"end_time\", \"total_active_seconds\", \"row_count\"]]\n",
    "collapsed = collapsed.sort_values([USER, \"start_time\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "collapsed.to_csv(OUT, index=False)\n",
    "print(f\"Wrote {OUT} ({len(collapsed)} rows)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d18ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Generate static JSON artifacts for the Internet Atlas frontend.\n",
    "\n",
    "Inputs:\n",
    "  - browsing CSV with columns:\n",
    "      panelist_id, used_at, active_seconds, domain, subdomain, full_domain\n",
    "\n",
    "Outputs (under --out /jsons):\n",
    "  - edges_u0_7.json\n",
    "  - edge_users_u0_7.json\n",
    "  - user_edges_u0_7/<userId>.json\n",
    "  - node_stats_u0_7.json\n",
    "\n",
    "Author: you\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ------------------------- Config -------------------------\n",
    "\n",
    "USER_MIN = 0\n",
    "USER_MAX = 7   # inclusive; 0..7 -> 8 users total (match file suffix u0_7)\n",
    "\n",
    "# Output filenames (match your static API expectations)\n",
    "EDGES_FILE = \"edges_u0_7.json\"\n",
    "EDGE_USERS_FILE = \"edge_users_u0_7.json\"\n",
    "USER_EDGES_DIR = \"user_edges_u0_7\"\n",
    "NODE_STATS_FILE = \"node_stats_u0_7.json\"\n",
    "\n",
    "\n",
    "# ------------------------- Helpers -------------------------\n",
    "\n",
    "def iso_utc(dt: pd.Series) -> pd.Series:\n",
    "    \"\"\"Convert pandas datetime to ISO-8601 Z strings.\"\"\"\n",
    "    # Localize naive to UTC (adjust if your data is already tz-aware)\n",
    "    if dt.dt.tz is None:\n",
    "        dt = dt.dt.tz_localize(\"UTC\")\n",
    "    return dt.dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "\n",
    "def normalize_domain(s: str) -> str:\n",
    "    return (s or \"\").strip().lower()\n",
    "\n",
    "\n",
    "def ensure_cols(df: pd.DataFrame, cols: List[str]):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "\n",
    "# ------------------------- Core steps -------------------------\n",
    "\n",
    "def load_and_filter(input_csv: str) -> pd.DataFrame:\n",
    "    \"\"\"Load CSV, parse/clean, filter users.\"\"\"\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # required columns\n",
    "    ensure_cols(df, [\"panelist_id\", \"used_at\", \"active_seconds\"])\n",
    "\n",
    "    # full_domain: build if not present from domain + subdomain\n",
    "    if \"full_domain\" not in df.columns:\n",
    "        ensure_cols(df, [\"domain\"])\n",
    "        sub = df[\"subdomain\"] if \"subdomain\" in df.columns else \"\"\n",
    "        df[\"full_domain\"] = (\n",
    "            sub.fillna(\"\").astype(str).str.strip().str.lower().replace(\"\", np.nan)\n",
    "            + \".\"\n",
    "            + df[\"domain\"].astype(str).str.strip().str.lower()\n",
    "        )\n",
    "        # Remove leading '.' when subdomain missing\n",
    "        df[\"full_domain\"] = df[\"full_domain\"].str.replace(\"^\\\\.\", \"\", regex=True)\n",
    "\n",
    "    # normalize critical fields\n",
    "    df[\"full_domain\"] = df[\"full_domain\"].astype(str).apply(normalize_domain)\n",
    "    df[\"panelist_id\"] = pd.to_numeric(df[\"panelist_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"active_seconds\"] = pd.to_numeric(df[\"active_seconds\"], errors=\"coerce\").fillna(0).astype(float)\n",
    "    df[\"used_at\"] = pd.to_datetime(df[\"used_at\"], errors=\"coerce\")\n",
    "\n",
    "    # filter desired users\n",
    "    df = df[df[\"panelist_id\"].between(USER_MIN, USER_MAX)].copy()\n",
    "\n",
    "    # drop rows with missing essentials\n",
    "    df = df.dropna(subset=[\"panelist_id\", \"used_at\", \"full_domain\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def collapse_sessions(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Collapse consecutive rows per user+domain into sessions:\n",
    "      start_time = first used_at\n",
    "      end_time   = max(used_at + active_seconds) within the run\n",
    "      total_active_seconds = sum(active_seconds) in the run\n",
    "    \"\"\"\n",
    "    USER, TS, DUR, FDOM = \"panelist_id\", \"used_at\", \"active_seconds\", \"full_domain\"\n",
    "\n",
    "    df = df.sort_values([USER, TS], kind=\"mergesort\").reset_index(drop=True)\n",
    "    df[\"_row_end\"] = df[TS] + pd.to_timedelta(df[DUR], unit=\"s\")\n",
    "\n",
    "    is_new_run = (df[USER].ne(df[USER].shift())) | (df[FDOM].ne(df[FDOM].shift()))\n",
    "    df[\"_run_id\"] = is_new_run.cumsum()\n",
    "\n",
    "    collapsed = (\n",
    "        df.groupby([USER, FDOM, \"_run_id\"], as_index=False)\n",
    "          .agg(\n",
    "              start_time=(TS, \"first\"),\n",
    "              end_time=(\"_row_end\", \"max\"),\n",
    "              total_active_seconds=(DUR, \"sum\"),\n",
    "              row_count=(\"_run_id\", \"size\"),\n",
    "          )\n",
    "    )\n",
    "    # Convert to ISO-8601 Z\n",
    "    collapsed[\"start_time\"] = iso_utc(collapsed[\"start_time\"])\n",
    "    collapsed[\"end_time\"]   = iso_utc(collapsed[\"end_time\"])\n",
    "\n",
    "    # Final columns\n",
    "    collapsed = collapsed[[USER, FDOM, \"start_time\", \"end_time\", \"total_active_seconds\", \"row_count\"]]\n",
    "    return collapsed\n",
    "\n",
    "\n",
    "def build_user_edges_from_sessions(sessions: pd.DataFrame) -> Dict[int, List[Tuple[str, str]]]:\n",
    "    \"\"\"\n",
    "    Build per-user edge sequences from sessions:\n",
    "    For each user, sort sessions by start_time and add a transition from\n",
    "    previous session's domain to current session's domain when it changes.\n",
    "    Returns: { userId: [(origin, target), ...] }\n",
    "    \"\"\"\n",
    "    USER, FDOM = \"panelist_id\", \"full_domain\"\n",
    "\n",
    "    # parse times back (for reliable sorting)\n",
    "    tmp = sessions.copy()\n",
    "    tmp[\"_start_dt\"] = pd.to_datetime(tmp[\"start_time\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "    user_edges: Dict[int, List[Tuple[str, str]]] = {}\n",
    "    for user_id, g in tmp.groupby(USER, sort=True):\n",
    "        g = g.sort_values(\"_start_dt\", kind=\"mergesort\")\n",
    "        domains = g[FDOM].tolist()\n",
    "        edges = []\n",
    "        prev = None\n",
    "        for d in domains:\n",
    "            if prev is not None and d != prev:\n",
    "                edges.append((prev, d))\n",
    "            prev = d\n",
    "        user_edges[int(user_id)] = edges\n",
    "    return user_edges\n",
    "\n",
    "\n",
    "def aggregate_edges(user_edges: Dict[int, List[Tuple[str, str]]]) -> Tuple[List[Dict], Dict[str, List[int]]]:\n",
    "    \"\"\"\n",
    "    Aggregate edges across users.\n",
    "    Returns:\n",
    "      - edges list with id, origin, target, num_users\n",
    "      - edge -> [userIds] map\n",
    "    \"\"\"\n",
    "    edge_to_users: Dict[Tuple[str, str], set] = {}\n",
    "    for uid, edges in user_edges.items():\n",
    "        for (o, t) in edges:\n",
    "            key = (normalize_domain(o), normalize_domain(t))\n",
    "            s = edge_to_users.setdefault(key, set())\n",
    "            s.add(uid)\n",
    "\n",
    "    # assign ids and build edges list\n",
    "    edges_list = []\n",
    "    edge_users_map: Dict[str, List[int]] = {}\n",
    "    eid = 1\n",
    "    for (o, t), users in edge_to_users.items():\n",
    "        edges_list.append({\n",
    "            \"id\": eid,\n",
    "            \"origin\": o,\n",
    "            \"target\": t,\n",
    "            \"num_users\": len(users)\n",
    "        })\n",
    "        edge_users_map[f\"{o}|{t}\"] = sorted(int(u) for u in users)\n",
    "        eid += 1\n",
    "\n",
    "    # stable sort edges (by num_users desc then alpha)\n",
    "    edges_list.sort(key=lambda e: (-e[\"num_users\"], e[\"origin\"], e[\"target\"]))\n",
    "    return edges_list, edge_users_map\n",
    "\n",
    "\n",
    "def build_node_stats_from_sessions(sessions: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Quick domain totals (used for overlay). For simplicity we compute domain-level\n",
    "    totals independent of mode, and populate both by_origin and by_target the same way.\n",
    "    \"\"\"\n",
    "    FDOM = \"full_domain\"\n",
    "\n",
    "    tmp = sessions.copy()\n",
    "    tmp[\"domain\"] = tmp[FDOM].apply(normalize_domain)\n",
    "\n",
    "    grp = tmp.groupby(\"domain\", as_index=False).agg(\n",
    "        visit_count=(\"domain\", \"size\"),\n",
    "        total_time_spent=(\"total_active_seconds\", \"sum\"),\n",
    "    )\n",
    "    grp[\"avg_time_per_visit\"] = grp[\"total_time_spent\"] / grp[\"visit_count\"]\n",
    "\n",
    "    # materialize dicts\n",
    "    stats_map = {\n",
    "        row[\"domain\"]: {\n",
    "            \"visit_count\": int(row[\"visit_count\"]),\n",
    "            \"total_time_spent\": float(row[\"total_time_spent\"]),\n",
    "            \"avg_time_per_visit\": float(row[\"avg_time_per_visit\"]),\n",
    "        }\n",
    "        for _, row in grp.iterrows()\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"by_origin\": stats_map,\n",
    "        \"by_target\": stats_map,  # same totals (you can split by edge-role later if desired)\n",
    "    }\n",
    "\n",
    "\n",
    "# ------------------------- Orchestrator -------------------------\n",
    "\n",
    "def main():\n",
    "    input = \"browsing.csv\"\n",
    "    out_dir =  \"new.csv\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    (out_dir / USER_EDGES_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"• Loading…\")\n",
    "    df = load_and_filter(input)\n",
    "\n",
    "    print(\"• Collapsing sessions…\")\n",
    "    sessions = collapse_sessions(df)\n",
    "\n",
    "    # If you also want to export sessions for inspection, uncomment:\n",
    "    # (out_dir / \"sessions_u0_7.json\").write_text(json.dumps({\n",
    "    #     \"results_count\": int(len(sessions)),\n",
    "    #     \"results\": sessions.to_dict(orient=\"records\")\n",
    "    # }, ensure_ascii=False))\n",
    "\n",
    "    print(\"• Building per-user edges…\")\n",
    "    user_edges = build_user_edges_from_sessions(sessions)\n",
    "\n",
    "    print(\"• Aggregating edges…\")\n",
    "    edges_list, edge_users_map = aggregate_edges(user_edges)\n",
    "\n",
    "    print(\"• Computing node stats…\")\n",
    "    node_stats = build_node_stats_from_sessions(sessions)\n",
    "\n",
    "    # ---- Write files ----\n",
    "    print(\"• Writing edges file:\", EDGES_FILE)\n",
    "    with open(out_dir / EDGES_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"results_count\": len(edges_list),\n",
    "            \"results\": edges_list\n",
    "        }, f, ensure_ascii=False)\n",
    "\n",
    "    print(\"• Writing edge users map:\", EDGE_USERS_FILE)\n",
    "    with open(out_dir / EDGE_USERS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(edge_users_map, f, ensure_ascii=False)\n",
    "\n",
    "    print(\"• Writing per-user edge sequences:\", USER_EDGES_DIR)\n",
    "    for uid, edges in user_edges.items():\n",
    "        rows = [{\n",
    "            \"id\": i + 1,\n",
    "            \"origin\": o,\n",
    "            \"target\": t,\n",
    "            \"num_users\": 1  # per-user sequence: single user\n",
    "        } for i, (o, t) in enumerate(edges)]\n",
    "        with open(out_dir / USER_EDGES_DIR / f\"{uid}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"results_count\": len(rows),\n",
    "                \"results\": rows\n",
    "            }, f, ensure_ascii=False)\n",
    "\n",
    "    print(\"• Writing node stats:\", NODE_STATS_FILE)\n",
    "    with open(out_dir / NODE_STATS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(node_stats, f, ensure_ascii=False)\n",
    "\n",
    "    print(\"✓ Done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
